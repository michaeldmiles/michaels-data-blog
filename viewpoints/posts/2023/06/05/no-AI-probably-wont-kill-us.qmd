---
title: "No, AI probably won’t kill us all -- and there’s more to this fear campaign than meets the eye"
description: |
  Last week, outlets around the world were plastered with news of yet another open letter claiming AI poses an existential threat to humankind. Michael Timothy Bennett argues that a healthy dose of scepticism is warranted when considering the 'AI doomsayer' narrative.     
categories:
  - AI
  - Large language models
  - Regulation
author: Michael Timothy Bennett
date: 06/05/2023
toc: false
image: images/AlanWarburton-SocialMedia.png
image-alt: A photographic rendering of a smiling face emoji seen through a refractive glass grid, overlaid with a diagram of a neural network.
---
Doomsaying is an old occupation. Artificial intelligence (AI) is a complex subject. It’s easy to fear what you don’t understand. These three truths go some way towards explaining the oversimplification and dramatisation plaguing discussions about AI.

Last week, outlets around the world were plastered with news of yet another <a href="https://www.safe.ai/statement-on-ai-risk">open letter claiming</a> AI poses an existential threat to humankind. This letter, published through the nonprofit Center for AI Safety, has been signed by industry figureheads including <a href="https://theconversation.com/ai-pioneer-geoffrey-hinton-says-ai-is-a-new-form-of-intelligence-unlike-our-own-have-we-been-getting-it-wrong-this-whole-time-204911">Geoffrey Hinton</a> and the chief executives of Google DeepMind, Open AI and Anthropic.

However, I’d argue a healthy dose of scepticism is warranted when considering the AI doomsayer narrative. Upon close inspection, we see there are commercial incentives to manufacture fear in the AI space.

And as a researcher of artificial general intelligence (AGI), it seems to me the framing of AI as an existential threat has more in common with 17th-century philosophy than computer science.

## Was ChatGPT a ‘breakthrough’?
When ChatGPT was released late last year, people were delighted, entertained and horrified.

But ChatGPT isn’t a research breakthrough as much as it is a product. The technology it is based on is several years old. An early version of its underlying model, GPT-3, was released in 2020 with many of the same capabilities. It just wasn’t easily accessible online for everyone to play with.

Back in 2020 and 2021, <a href="https://ieeexplore.ieee.org/document/9495946">I</a> and many <a href="https://link.springer.com/article/10.1007/s11023-020-09548-1">others</a> wrote papers discussing the capabilities and shortcomings of GPT-3 and similar models – and the world carried on as always. Forward to today, and ChatGPT has had an incredible impact on society. What changed?

In March, Microsoft researchers <a href="https://futurism.com/gpt-4-sparks-of-agi">published a paper</a> claiming GPT-4 showed “sparks of artificial general intelligence”. AGI is the subject of a variety of competing definitions, but for the sake of simplicity can be understood as AI with human-level intelligence.

Some immediately interpreted the Microsoft research as saying GPT-4 <em>is</em> an AGI. By the definitions of AGI I’m familiar with, this is certainly not true. Nonetheless, it added to the hype and furore, and it was hard not to get caught up in the panic. Scientists are no more immune to <a href="https://link.springer.com/book/10.1007/978-3-030-36822-7">group think</a> than anyone else.

The same day that paper was submitted, The Future of Life Institute <a href="https://futureoflife.org/open-letter/pause-giant-ai-experiments/">published an open letter</a> calling for a six-month pause on training AI models more powerful than GPT-4, to allow everyone to take stock and plan ahead. Some of the AI luminaries who signed it expressed concern that AGI poses an existential threat to humans, and that ChatGPT is too close to AGI for comfort.

Soon after, prominent AI safety researcher Eliezer Yudkowsky – who has been commenting on the dangers of superintelligent AI <a href="https://intelligence.org/files/AIPosNegFactor.pdf">since well before</a> 2020 – took things a step further. <a href="https://time.com/6266923/ai-eliezer-yudkowsky-open-letter-not-enough/">He claimed</a> we were on a path to building a “superhumanly smart AI”, in which case “the obvious thing that would happen” is “literally everyone on Earth will die”. He even suggested countries need to be willing to risk nuclear war to enforce compliance with AI regulation across borders.

## I don’t consider AI an imminent existential threat

One aspect of AI safety research is to address potential dangers AGI might present. It’s a difficult topic to study because there is little agreement on what intelligence is and how it functions, let alone what a superintelligence might entail. As such, researchers must rely as much on speculation and philosophical argument as on evidence and mathematical proof.

There are two reasons I’m not concerned by ChatGPT and its <a href="https://lablab.ai/blog/what-is-babyagi-and-how-can-i-benefit-from-it">byproducts</a>.

First, it isn’t even close to the sort of artificial superintelligence that might conceivably pose a threat to humankind. The models underpinning it are slow learners that require immense volumes of data to construct anything akin to the versatile concepts humans can concoct from only a few examples. In this sense, it is not “intelligent”.

Second, many of the more catastrophic AGI scenarios depend on premises I find implausible. For instance, there seems to be a prevailing (but unspoken) assumption that sufficient intelligence amounts to limitless real-world power. If this was true, more scientists would be billionaires.

Moreover, cognition as we understand it in humans takes place as part of a physical environment (which includes our bodies), and this environment imposes limitations. The concept of AI as a “software mind” unconstrained by hardware has more in common with 17th-century <a href="https://plato.stanford.edu/entries/dualism/">dualism</a> (the idea that the mind and body are separable) than with contemporary theories of the mind existing as <a href="https://plato.stanford.edu/entries/embodied-cognition/">part of the physical world</a>.

## Why the sudden concern?

Still, doomsaying is old hat, and the events of the last few years probably haven’t helped – but there may be more to this story than meets the eye.

Among the prominent figures calling for AI regulation, many work for or have ties to incumbent AI companies. This technology is useful, and there is money and power at stake – so fearmongering presents an opportunity.

Almost everything involved in building ChatGPT has been published in research anyone can access. OpenAI’s competitors can (and have) replicated the process, and it won’t be long before free and open-source alternatives flood the market.

This point was made clearly in a memo <a href="https://www.semianalysis.com/p/google-we-have-no-moat-and-neither">purportedly leaked</a> from Google entitled “We have no moat, and neither does OpenAI”. A moat is jargon for a way to secure your business against competitors.

Yann LeCun, who leads AI research at Meta, says these models should be open since they will become public infrastructure. He and many others are <a href="https://www.businesstoday.in/technology/news/story/completely-ridiculous-metas-chief-ai-scientist-yann-lecun-dismisses-elon-musks-civilisation-destruction-fear-383371-2023-05-30">unconvinced by the AGI doom</a> narrative.

<center><blockquote class="twitter-tweet"><p lang="en" dir="ltr" >A NYT article on the debate around whether LLM base models should be closed or open.<br><br>Meta argues for openness, starting with the release of LLaMA (for non-commercial use), while OpenAI and Google want to keep things closed and proprietary.<br><br>They argue that openness can be…</p>&mdash; Yann LeCun (@ylecun) <a href="https://twitter.com/ylecun/status/1659172655663030272?ref_src=twsrc%5Etfw">May 18, 2023</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></center>

Notably, <a href="https://fortune.com/2023/05/05/meta-mark-zuckerberg-not-invited-ai-meeting-white-house/">Meta wasn’t invited</a> when US President Joe Biden recently met with the leadership of Google DeepMind and OpenAI. That’s despite the fact that Meta is almost certainly a leader in AI research; it produced PyTorch, the machine-learning framework OpenAI used to make GPT-3.

At the White House meetings, OpenAI chief executive Sam Altman suggested the US government should issue licences to those who are trusted to responsibly train AI models. Licences, as Stability AI chief executive Emad Mostaque <a href="https://twitter.com/EMostaque/status/1658653142429450242?s=20">puts it</a>, “are a kinda moat”.

Companies such as Google, OpenAI and Microsoft have everything to lose by allowing small, independent competitors to flourish. Bringing in licensing and regulation would help cement their position as market leaders and hamstring competition before it can emerge.

While regulation is appropriate in some circumstances, regulations that are rushed through will favour incumbents and suffocate small, <a href="https://www.forbes.com/sites/hessiejones/2023/04/19/amid-growing-call-to-pause-ai-research-laion-petitions-governments-to-keep-agi-research-open-active-and-responsible/?sh=1b21161a62e3">free and open-source competition</a>.

<center><blockquote class="twitter-tweet"><p lang="en" dir="ltr">Think Google or Microsoft are encouraging legislation for your safety? But of course! These are honorable companies.<br><br>You might think they&#39;d like less competition too though. Maybe a monopoly? Maybe legal red tape preventing free and open source alternatives? Perhaps other… <a href="https://t.co/Z7vSpMyuHg">https://t.co/Z7vSpMyuHg</a></p>&mdash; Michael Timothy Bennett (@MiTiBennett) <a href="https://twitter.com/MiTiBennett/status/1654357631514079233?ref_src=twsrc%5Etfw">May 5, 2023</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></center>

<!-- Below is The Conversation's page counter tag. Please DO NOT REMOVE. -->

<img src="https://counter.theconversation.com/content/206614/count.gif?distributor=republish-lightbox-basic" alt="The Conversation" width="1" height="1" style="border: none !important; box-shadow: none !important; margin: 0 !important; max-height: 1px !important; max-width: 1px !important; min-height: 1px !important; min-width: 1px !important; opacity: 0 !important; outline: none !important; padding: 0 !important" referrerpolicy="no-referrer-when-downgrade" />

<!-- End of code. If you don't see any code above, please get new code from the Advanced tab after you click the republish button. The page counter does not collect any personal data. More info: https://theconversation.com/republishing-guidelines -->

::: {.article-btn}
[Discover more Viewpoints](/viewpoints/index.qmd)
:::

::: {.further-info}
::: grid
::: {.g-col-12 .g-col-md-12}
About the author
: <a href="https://theconversation.com/profiles/michael-timothy-bennett-1283108">Michael Timothy Bennett</a> is a PhD student in the School of Computing, <a href="https://theconversation.com/institutions/australian-national-university-877">Australian National University</a>.
:::
::: {.g-col-12 .g-col-md-12}
Copyright and licence
: This article is republished from <a href="https://theconversation.com">The Conversation</a> under a Creative Commons license. Read the <a href="https://theconversation.com/no-ai-probably-wont-kill-us-all-and-theres-more-to-this-fear-campaign-than-meets-the-eye-206614">original article</a>.

: Thumbnail image by <a href="https://alanwarburton.co.uk/">Alan Warburton</a> / © BBC / <a href="https://www.betterimagesofai.org">Better Images of AI</a> / Social Media / <a href="https://creativecommons.org/licenses/by/4.0/">Licenced by CC-BY 4.0</a>.
:::
:::
:::